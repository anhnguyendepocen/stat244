\documentclass{tufte-book}

\usepackage{amsmath, amsthm}
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title{STATS 244 \\ Homework 4}
\author{Joe Seidel}
\date{\today}

\usepackage{booktabs}
\usepackage{units}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage{pdfpages}
\usepackage{tikz}
\usepackage{wasysym}

\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\DeclareMathOperator{\proj}{proj}
\newcommand{\vct}{\mathbf}


\newcommand{\dprod}[2]{\langle #1, #2 \rangle}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newtheoremstyle{mytheoremstyle} % name
	{\topsep}		% Space above
	{\topsep}		% Space below
	{\itshape}		% Body font
	{}			% Indent amount
	{\bfseries}	% Theorem head font
	{\textnormal{:}}	% Punctuation after theorem head
	{.5em}		% Space after theorem head
	{}			%Theorem headspec
\theoremstyle{mytheoremstyle}
\newtheorem*{thm}{Thm.}

\newtheoremstyle{mylemstyle} % name
	{\topsep}		% Space above
	{\topsep}		% Space below
	{\itshape}		% Body font
	{}			% Indent amount
	{\bfseries}	% Theorem head font
	{\textnormal{:}}	% Punctuation after theorem head
	{.5em}		% Space after theorem head
	{}			%Theorem headspec
\theoremstyle{mylemstyle}
\newtheorem*{lem}{Lem.}


\newtheoremstyle{mydefstyle} % name
	{\topsep}		% Space above
	{\topsep}		% Space below
	{\normalfont}	% Body font
	{}			% Indent amount
	{\bfseries}	% Theorem head font
	{\textnormal{:}}	% Punctuation after theorem head
	{.5em}		% Space after theorem head
	{}			%Theorem headspec
\theoremstyle{mydefstyle}
\newtheorem*{mydef}{Def.}
\newtheorem*{ex}{E.g.}

\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\subsection{Question 1}
Let $Z \sim N(0,1)$, a stand normal distribution and let $X \sim N(\mu \sigma^2)$.  Let $\Phi(z)$ be the cdf of $Z$.  Suppose $X \sim N(-4, 16)$; find.

It helps to know that
\[ P(X < x) = \Phi\Big(\frac{x-\mu}{\sigma}\Big). \]

\begin{enumerate}
\item $P(X > 2)$.
\begin{align*}
P(X > 2) &= 1 - P(X < 2)
&= 1 - \Phi\Big(\frac{2- (-4)}{\sqrt{16}}\Big)\\
&= 1 - .9332\\
&= .0688\\
\end{align*}

\item $P(0 < X < 4)$
\begin{align*}
    P(0 < X < 4) &= P(X<4) - P(X<0)\\
    &= \Phi\Big(\frac{4 -(-4)}{4}\Big) - \Phi\Big(\frac{0 -(-4)}{4}\Big) \\
    &= .1539
\end{align*}

\item $P(|X+3| \geq 3)$
\begin{align*}
    P(|X+3| \geq 3) &= P(X \geq 0) + P(X \leq -6)\\
    &= 1-\Phi\Big(\frac{4}{4}\Big) + \Phi\Big(\frac{-2}{4}\Big)\\
    &= .1587 + .3085\\
    &= .4672
\end{align*}

\item $P(X \leq 0 \text{ or } X \geq 3)$
\begin{align*}
    P(X \leq 0 \text{ or } X \geq 3) &= P(X \leq 0) + P(X \geq 3)\\
    &= \Phi\Big(\frac{4}{4}\Big) + 1 - \Phi\Big(\frac{7}{4}\Big)\\
    &= .8413 + .0003\\
    &= .8416\\
\end{align*}

\end{enumerate}


\subsection{Question 2}
Based on student A's performace during the first two weeks of a course, the professor has approximately a normal $N(70, 8^2)$ prior distribution about the student's true ability, on a scale of 0 to 100.  Consider the midterm examination as an error-prone measure of the student's true ability, where if the true ability is $x$, the examination score can be modeled as approximately normally distributed, $N(x,6^2)$.  The student scores 90 on the midterm.

\begin{enumerate}
\item What are the posterior expectaion and the probability that the student's true ability is above 85?

\newthought{We have} the following information.
    \[f(\theta) \sim N(70, 8^2) \]
 and
    \[f(x\mid \theta)\sim N(x, 6^2)\]

Using Bayes, $f(\theta\mid X) \propto f(x\mid \theta)f(\theta)$ the posterior distribution is

\[ (\theta\mid X) = \frac{1}{\sqrt{2\pi}B}e^{\frac{(\theta-A)^2}{2B^2}} \]
Where
\[ A = \frac{6^2(70) + 8^2(90)}{6^2 + 8^2} \]
and
\[B^2 = \frac{6^2*8^2}{6^2 + 8^2} \]

Which give us a $N(82.8, 23.4)$ posterier distribution and means posterior of expectation of the student's true ability it $82.8$.  Using the the method from question 1, there is a .4625 probability that the students true ability is above $85$.

\item Above 90?

.3792

\end{enumerate}


\subsection{Question 3}
A "psychic" uses a five-card deck of cards to demenonstrate ESP, and claims to be able to guess a card correctly with probability .5.  A single experiment consists of making five guesses, reshuffling the deck after each guess.  The experiment is treid and the "pyschic" guesses correctly 3 times out of give.  Assuming the only two possibilities are "ESP" and "ordinary guessing", how how must the a priori ability be that "psychic" has ESP is at alteast .7?

\newthought{Given} that the psychic claims probability of .5 we can call this $f(\theta) \sim Beta(1,1)$ and $f(x\mid \theta) \sim Bin(5, \theta)$  From, observing $k=3$ correct guesses we get the result that $f(\theta \mid x) \sim Bin(4,3)$.  However, we want to know what the prior probability would have needed to be in order for the pyschic to have atleast $.7$.

To have a probability of atleast $.7$ we'd need something like $Beta(7,3)$ distribution.  So the a priori distribution should be something like $Beta(4,1)$ which means atleast .8 probability is required.

\subsection{Question 4}

Suppose $\hat{\theta}_1$ and $\hat{\theta}_2$ are uncorrelated and both are unbiased estimators of $\theta$, and that $\Var(\hat{\theta}_1) = 2\Var(\hat{\theta}_2)$.

\begin{enumerate}

\item Show that for any constant $c$, the weighted average $\hat{\theta}_3 = c\hat{\theta}_1 + (1-c)\hat{\theta}_2$ is an unbiased estimator of $\theta$.

\begin{align*}
E(\hat{\theta}_3) &= cE(\hat{\theta}_1) + (1-c)E(\hat{\theta}_2)\\
&= c\theta + (1-c)\theta\\
&= \theta\\
\end{align*}

Hence $B(\hat{\theta}_3) = 0$.

\item Find $c$ for which $\hat{\theta}_3$ has the smallest MSE.

The MSE of $\hat{\theta}_3$ is

\[ c^2Var(\hat{\theta}_1) + (1-c)^2\Var(\hat{\theta}_2) \]

We can find the value, $c$, that minimizes MSE by differentation.

\begin{align*}
\frac{d}{dc} MSE(\hat{\theta}_2) &= 2cVar(\hat{\theta_1}) - 2Var(\hat{\theta_2})(1-c)\\
&= 2cVar(\hat{\theta_1}) - Var(\hat{\theta_1})(1-c)\\
&= 3cVar(\hat{\theta_1}) - Var(\hat{\theta_1})\\
\end{align*}

Setting the above equal to $0$ results in $c=\frac{1}{3}$.  Furthermore, the second derivative of the MSE is positive so we can confirm that $c$ minimizes.

\item Are there any values of $c$, $0 \leq c \leq 1$ for which $\hat{\theta}_3$ is better (in the sense of MSE) than both $\hat{\theta}_1$ and $\hat{\theta}_2$

\newthought{We are} given that $\Var(\hat{\theta}_2)$ is less than $\Var(\hat{\theta}_1)$.  Since the bias as zero, the MSE is just the variance of each estimator.

\begin{align*}
\text{MSE }\hat{\theta}_3 &= c^2Var(\hat{\theta}_1) + (1-c)^2\Var(\hat{\theta}_2)\\
&= c^2 2\Var(\hat{\theta}_2) + (1-c)^2\Var(\hat{\theta}_2)\\
&=
\end{align*}

\end{enumerate}

\end{document}
\grid
